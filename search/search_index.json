{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"base/jira/","title":"Jira","text":""},{"location":"base/jira/#_1","title":"\u5b89\u88c5\u624b\u518c","text":""},{"location":"base/jira/#docker-jira","title":"docker \u90e8\u7f72 jira","text":"<pre><code>hostnamectl set-hostname --static jira\necho \"192.168.0.117 jira\" &gt;&gt; /etc/hosts\n</code></pre> <pre><code>docker pull atlassian/jira-software:8.16.1\ndocker pull mysql:8.0.22\n</code></pre> <p><pre><code>docker run -e \"MYSQL_ROOT_PASSWORD=123123\" -d --name mysql -p 3306:3306 -v ./mysql/data:/var/lib/mysql mysql:8.0.22\ndocker exec -it mysql mysql -uroot -p123456 -e \"CREATE DATABASE jiradb CHARACTER SET utf8mb4 COLLATE utf8mb4_bin;\"\ndocker exec -it mysql mysql -uroot -p123456 -e \"SHOW DATABASES;\"\nmysql: [Warning] Using a password on the command line interface can be insecure.\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| jiradb             |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n</code></pre> <pre><code>mkdir -pv  /data/apps/jira\ndocker run -d --name jira -p 8080:8080 -v /data/apps/jira/data:/var/atlassian/application-data/jira atlassian/jira-software:8.16.1\n</code></pre></p>"},{"location":"k8s/installation/kubeadm-v1.26/","title":"Kubeadm \u5b89\u88c5 Kubernetes v1.26.9(Containerd)","text":""},{"location":"k8s/installation/kubeadm-v1.26/#_1","title":"\u4e3b\u673a\u73af\u5883\u51c6\u5907","text":"<p>\u4f7f\u7528Kubeadm\u5feb\u901f\u90e8\u7f72Kubernetes\u96c6\u7fa4\uff0c\u64cd\u4f5c\u7cfb\u7edf\u4e3aUbuntu 20.04.6 LTS\uff0c\u7528\u5230\u7684\u5404\u76f8\u5173\u7a0b\u5e8f\u7248\u672c\u5982\u4e0b</p> <ul> <li>Kubernetes: v1.26.9</li> <li>Containerd: 1.7.27</li> <li>CNI: Flannel</li> </ul>"},{"location":"k8s/installation/kubeadm-v1.26/#1","title":"1. \u4e3b\u673a\u540d\u89e3\u6790","text":"<pre><code>echo \"\n192.168.1.31 k8s-master01 master01 k8s-api.linux.io\n192.168.1.41 k8s-worker01 worker01\n192.168.1.42 k8s-worker02 worker02\n\" |sudo tee -a /etc/hosts\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#2","title":"2. \u7cfb\u7edf\u57fa\u7840\u8f6f\u4ef6\u73af\u5883","text":"<pre><code>sudo apt-get update\nsudo apt remove ufw lxd lxcfs lxc-common -y\nsudo apt install -y bash-completion conntrack  ipset ipvsadm  jq libseccomp2  nfs-common  psmisc  rsync  socat\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#3-swap","title":"3. \u7981\u7528\u7cfb\u7edfswap","text":"<pre><code>sudo sed  -ri  's@/.*swap.*@# &amp;@' /etc/fstab \nsudo systemctl  mask $(sudo systemctl  --type swap|grep \"loaded active\"|awk '{print $1}')\nsudo swapoff -a &amp;&amp; sudo sysctl -w vm.swappiness=0\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#4","title":"4. \u65f6\u95f4\u540c\u6b65","text":"<pre><code>sudo cp /usr/share/zoneinfo/Asia/Shanghai  /etc/localtime\nsudo apt install  -y chrony\n\necho \"\nserver ntp.aliyun.com iburst\nstratumweight 0\ndriftfile /var/lib/chrony/drift\nrtcsync\nmakestep 10 3\nbindcmdaddress 127.0.0.1\nbindcmdaddress ::1\nkeyfile /etc/chrony.keys\ncommandkey 1\ngeneratecommandkey\nlogchange 0.5\nlogdir /var/log/chrony\n\"| sudo tee /etc/chrony/chrony.conf\n\nsudo systemctl  restart chrony\nchronyc  sources\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#5","title":"5.  \u4f18\u5316\u65e5\u5fd7","text":"<p>\u4f18\u5316\u8bbe\u7f6e journal \u65e5\u5fd7\u76f8\u5173\uff0c\u907f\u514d\u65e5\u5fd7\u91cd\u590d\u641c\u96c6\uff0c\u6d6a\u8d39\u7cfb\u7edf\u8d44\u6e90</p> <pre><code>sudo mkdir -pv /etc/systemd/journald.conf.d\n\necho \"[Journal]\n# \u6301\u4e45\u5316\u4fdd\u5b58\u5230\u78c1\u76d8\nStorage=persistent\n# \u6700\u5927\u5360\u7528\u7a7a\u95f4 2G\nSystemMaxUse=2G\n# \u5355\u65e5\u5fd7\u6587\u4ef6\u6700\u5927 200M\nSystemMaxFileSize=200M\n# \u65e5\u5fd7\u4fdd\u5b58\u65f6\u95f4 2 \u5468\nMaxRetentionSec=2week\n# \u7981\u6b62\u8f6c\u53d1\nForwardToSyslog=no\nForwardToWall=no\"|sudo tee  /etc/systemd/journald.conf.d/95-k8s-journald.conf\n\nsudo systemctl restart systemd-journald \n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#6","title":"6. \u81ea\u52a8\u52a0\u8f7d\u6a21\u5757","text":"<p>\u542f\u7528systemd\u81ea\u52a8\u52a0\u8f7d\u6a21\u5757\u670d\u52a1</p> <pre><code>echo \"\nbr_netfilter\nip_vs\nip_vs_rr\nip_vs_wrr\nip_vs_sh\nnf_conntrack\n\" |sudo tee /etc/modules-load.d/10-k8s-modules.conf\nsudo systemctl enable systemd-modules-load  --now\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#7","title":"7. \u5185\u6838\u53c2\u6570\u4f18\u5316","text":"<pre><code>echo \"\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-arptables = 1\nnet.ipv4.tcp_tw_reuse = 0\nnet.core.somaxconn = 32768\nnet.netfilter.nf_conntrack_max=1000000\nvm.swappiness = 0\nvm.max_map_count=655360\nfs.file-max=6553600\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_intvl = 30\nnet.ipv4.tcp_keepalive_probes = 10\n\" | sudo tee /etc/sysctl.d/95-k8s-sysctl.conf\n\nsudo sysctl -p /etc/sysctl.d/95-k8s-sysctl.conf\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#8-ulimits","title":"8. \u8bbe\u7f6e\u7cfb\u7edf ulimits","text":"<pre><code>sudo mkdir /etc/systemd/system.conf.d\n\necho \"\n[Manager]\nDefaultLimitCORE=infinity\nDefaultLimitNOFILE=100000\nDefaultLimitNPROC=100000\n\"| sudo tee /etc/systemd/system.conf.d/30-k8s-ulimits.conf\n</code></pre> <ul> <li>\u628aSCTP\u5217\u5165\u5185\u6838\u6a21\u5757\u9ed1\u540d\u5355</li> </ul> <pre><code>echo \"\ninstall sctp /bin/true\n\"|sudo tee /etc/modprobe.d/sctp.conf\n</code></pre> <ul> <li>\u91cd\u542f\u673a\u5668</li> </ul> <pre><code>sudo reboot\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#_2","title":"\u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u65f6","text":"<pre><code>for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done\n\n# \u5b89\u88c5\u5fc5\u8981\u7684\u4e00\u4e9b\u7cfb\u7edf\u5de5\u5177\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\n\n# \u4fe1\u4efb Docker \u7684 GPG \u516c\u94a5\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# \u5199\u5165\u8f6f\u4ef6\u6e90\u4fe1\u606f\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# \u5b89\u88c5 containerd\nsudo apt update\nsudo apt install -y containerd.io\n\n# \u4f7f\u7528systemd\u4f5c\u4e3acgroup\ncontainerd config default | sudo tee /etc/containerd/config.toml &gt;/dev/null 2&gt;&amp;1\nsudo sed -i 's@SystemdCgroup \\= false@SystemdCgroup \\= true@g' /etc/containerd/config.toml\nsudo sed -i 's@registry.k8s.io/pause:3.8@registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9@g' /etc/containerd/config.toml\n\n# \u542f\u52a8\u670d\u52a1\u5e76\u8bbe\u7f6e\u4e3a\u81ea\u542f\u52a8\nsudo systemctl restart containerd\nsudo systemctl enable containerd\n</code></pre> <pre><code>sudo mkdir /etc/systemd/system/containerd.service.d\necho \"\n[Service]\nEnvironment=\"HTTP_PROXY=http://192.168.1.102:7890\"  \nEnvironment=\"HTTPS_PROXY=http://192.168.1.102:7890\" \nEnvironment=\"NO_PROXY=localhost,127.0.0.0/8,10.244.0.0/16,172.16.0.0/12,10.96.0.0/12,.svc,.cluster.local,.linux.io\"\n\"| sudo tee /etc/systemd/system/containerd.service.d/http-proxy.conf\n\nsudo systemctl daemon-reload\nsudo systemctl restart containerd.service\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#kubeadm","title":"\u5b89\u88c5kubeadm","text":"<pre><code># \u8bbe\u7f6e\u963f\u91cckubernetres\u6e90\ncurl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/kubernetes-archive-keyring.gpg &gt;/dev/null\n\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# \u5b89\u88c5kubeadm\nsudo apt update\nsudo apt install -y kubeadm=1.26.9-00 kubelet=1.26.9-00 kubectl=1.26.9-00\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#_3","title":"\u521d\u59cb\u5316\u96c6\u7fa4","text":"<pre><code>sudo kubeadm config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --kubernetes-version=1.26.9\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#1_1","title":"1. \u63a7\u5236\u5e73\u9762\u521d\u59cb\u5316","text":"<pre><code>sudo kubeadm init --kubernetes-version=v1.26.9 \\\n    --control-plane-endpoint=k8s-api.linux.io \\\n    --apiserver-advertise-address=0.0.0.0 \\\n    --pod-network-cidr=10.244.0.0/16   \\\n    --service-cidr=10.96.0.0/12 \\\n    --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\\n    --token-ttl=0 \\\n    --upload-certs \\\n    --ignore-preflight-errors=Swap\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n  kubeadm join k8s-api.linux.io:6443 --token 6dg4xz.zxima0m81fw7ikfg \\\n    --discovery-token-ca-cert-hash sha256:60108201f09d1acc5ad9f010baadb4bd4240fd9e7a44d06c18ce6fe6b8b4aa87 \\\n    --control-plane --certificate-key 3b63422672a10b4cd313bd9a32516f1cac4a240f6939a74144f5d0c666a7a1dc\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n\"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join k8s-api.linux.io:6443 --token 6dg4xz.zxima0m81fw7ikfg \\\n    --discovery-token-ca-cert-hash sha256:60108201f09d1acc5ad9f010baadb4bd4240fd9e7a44d06c18ce6fe6b8b4aa87\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#2-kubectl","title":"2. \u914d\u7f6eKubectl\u5ba2\u6237\u7aef","text":"<pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#3-flannel","title":"3. \u90e8\u7f72Flannel","text":"<pre><code>wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\nkubectl apply -f kube-flannel.yml\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#4-worker","title":"4. \u6dfb\u52a0Worker\u8282\u70b9","text":"<pre><code>sudo kubeadm join k8s-api.linux.io:6443 --token 6dg4xz.zxima0m81fw7ikfg \\\n    --discovery-token-ca-cert-hash sha256:60108201f09d1acc5ad9f010baadb4bd4240fd9e7a44d06c18ce6fe6b8b4aa87\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#ipvs","title":"\u4fee\u6539\u7f51\u7edc\u6a21\u5f0fIPVS","text":"<pre><code>~$ kubectl  edit cm/kube-proxy -n kube-system -o yaml\n  ...\n  metricsBindAddress: \"\"\n  mode: \"ipvs\" #\n  nodePortAddresses: null\n  oomScoreAdj: null\n  ...\n\n~$ kubectl  delete pod -n kube-system -l k8s-app=kube-proxy\n~$ sudo ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.96.0.1:443 rr\n  -&gt; 192.168.1.31:6443            Masq    1      0          0\nTCP  10.96.0.10:53 rr\n  -&gt; 10.244.1.2:53                Masq    1      0          0\n  -&gt; 10.244.1.3:53                Masq    1      0          0\nTCP  10.96.0.10:9153 rr\n  -&gt; 10.244.1.2:9153              Masq    1      0          0\n  -&gt; 10.244.1.3:9153              Masq    1      0          0\nUDP  10.96.0.10:53 rr\n  -&gt; 10.244.1.2:53                Masq    1      0          0\n  -&gt; 10.244.1.3:53                Masq    1      0          0\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.26/#_4","title":"\u9a8c\u8bc1\u96c6\u7fa4","text":"<ul> <li>\u521b\u5efa Deployment \u548c Service \u8d44\u6e90</li> </ul> <pre><code>~$ kubectl  get svc/myapp\nNAME    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nmyapp   NodePort   10.97.153.71   &lt;none&gt;        80:30488/TCP   9s\n~$ kubectl  get pod -o wide\nNAME                     READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES\nmyapp-6454786f57-cx7wm   1/1     Running   0          23s   10.244.2.2   k8s-worker02   &lt;none&gt;           &lt;none&gt;\nmyapp-6454786f57-fs8fz   1/1     Running   0          23s   10.244.1.4   k8s-worker01   &lt;none&gt;           &lt;none&gt;\nmyapp-6454786f57-kl64l   1/1     Running   0          23s   10.244.2.3   k8s-worker02   &lt;none&gt;           &lt;none&gt;\n</code></pre> <pre><code>for i in `seq 5`;do curl http://10.97.153.71/hostname.html;done\nmyapp-6454786f57-kl64l\nmyapp-6454786f57-cx7wm\nmyapp-6454786f57-fs8fz\nmyapp-6454786f57-kl64l\nmyapp-6454786f57-cx7wm\n</code></pre> <p>\u6ce8\u610f\uff1aService IP \u548c NodePort \u4ec5\u662f\u7f51\u7edc\u89c4\u5219\uff0c\u6240\u4ee5\u4e0d\u652f\u6301ping\uff0c \u4f46\u662f\u652f\u6301telnet</p> <ul> <li>\u9a8c\u8bc1\u7f51\u7edc</li> </ul> <p>service \u7684\u9ed8\u8ba4\u57df\u540d\u4e3a SVC_NAME.NAMESPACE.svc.cluster.local  <pre><code>~$ kubectl  run busybox --image=busybox:1.28 -- sleep 3600\n\n~$ kubectl  exec -it busybox -- nslookup myapp.default.svc.cluster.local\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      myapp.default.svc.cluster.local\nAddress 1: 10.97.153.71 myapp.default.svc.cluster.local\n</code></pre></p>"},{"location":"k8s/installation/kubeadm-v1.28/","title":"Kubeadm \u5b89\u88c5 Kubernetes v1.28.10(Docker)","text":""},{"location":"k8s/installation/kubeadm-v1.28/#_1","title":"\u4e3b\u673a\u73af\u5883\u51c6\u5907","text":"<p>\u4f7f\u7528Kubeadm\u5feb\u901f\u90e8\u7f72Kubernetes\u96c6\u7fa4\uff0c\u64cd\u4f5c\u7cfb\u7edf\u4e3aUbuntu 20.04.6 LTS\uff0c\u7528\u5230\u7684\u5404\u76f8\u5173\u7a0b\u5e8f\u7248\u672c\u5982\u4e0b</p> <p>Kubernetes: v1.28.6 Docker: 24.0.8 Cri-dockerd: v0.3.8</p>"},{"location":"k8s/installation/kubeadm-v1.28/#1","title":"1. \u4e3b\u673a\u540d\u89e3\u6790","text":"<pre><code>cat &gt;&gt; /etc/hosts &lt;&lt; 'EOF'\n192.168.1.201 k8s-master01\n192.168.1.211 k8s-worker01\n192.168.1.212 k8s-worker02\n192.168.1.213 k8s-worker03\n# k8s-vip \u9884\u7559\u540e\u671f\u6269\u5c55\u9ad8\u53ef\u7528\u96c6\u7fa4\n192.168.1.201 k8s-vip\nEOF\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#2-selinuxubuntuselinux","title":"2. \u5173\u95ed\u9632\u706b\u5899\u548cselinux(Ubuntu\u4e0d\u7528\u7ba1SELinux)","text":"<pre><code>systemctl disable --now ufw\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#3-swap","title":"3. \u5173\u95edswap\u5206\u533a","text":"<pre><code>sed -i 's/.*swap.*/#&amp;/' /etc/fstab\nswapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsystemctl  mask swap.target\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#4","title":"4. \u65f6\u533a\u8ddf\u65f6\u95f4\u540c\u6b65","text":"<pre><code>timedatectl  set-timezone Asia/Shanghai\n\napt install -y chrony\ncat  &gt; /etc/chrony/chrony.conf &lt;&lt; 'EOF'\npool ntp.aliyun.com       iburst maxsources 4\nkeyfile /etc/chrony/chrony.keys\ndriftfile /var/lib/chrony/chrony.drift\nlogdir /var/log/chrony\nmaxupdateskew 100.0\nrtcsync\nmakestep 1 3\nEOF\nsystemctl restart chrony.service &amp;&amp; systemctl  enable chrony.service\n\nchronyc sources\n210 Number of sources = 1\nMS Name/IP address         Stratum Poll Reach LastRx Last sample\n===============================================================================\n^* 203.107.6.88                  2   6    37    14   +450us[ +336us] +/-   30ms\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#5-ipvs","title":"5. \u52a0\u8f7dIPVS\u6a21\u5757","text":"<pre><code>apt install ipset ipvsadm -y\n\ntee  /etc/modules-load.d/k8s.conf &lt;&lt; 'EOF'\nbr_netfilter\nip_vs\nip_vs_rr\nip_vs_wrr\nip_vs_sh\nnf_conntrack\nnf_nat\noverlay\nvxlan\nEOF\nsystemctl restart systemd-modules-load\nlsmod | grep -E 'br_netfilter|ip_vs|nf_conntrack|overlay|vxlan'\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#6","title":"6. \u5185\u6838\u53c2\u6570\u4f18\u5316","text":"<pre><code>tee  /etc/sysctl.d/k8s.conf &lt;&lt; 'EOF'\n# \u5f00\u542fIPv4\u8f6c\u53d1\nnet.ipv4.ip_forward = 1\n# \u5141\u8bb8\u6865\u63a5\u7684\u6d41\u91cf\u8fdb\u5165iptables/netfilter\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\n# \u4f18\u5316\u8fde\u63a5\u8ddf\u8e2a\u8868\u5927\u5c0f\uff0c\u9632\u6b62\u5927\u89c4\u6a21\u8fde\u63a5\u7206\u6389\nnet.netfilter.nf_conntrack_max = 2310720\n# TCP\u4f18\u5316\uff08\u7f29\u77ed TIME_WAIT\uff0c\u5feb\u901f\u56de\u6536\u8fde\u63a5\uff09\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_fin_timeout = 15\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_intvl = 30\nnet.ipv4.tcp_keepalive_probes = 5\n# \u5185\u5b58\u76f8\u5173\u4f18\u5316\uff08\u9632\u6b62OOM\uff09\nvm.swappiness = 0\nvm.overcommit_memory = 1\nvm.panic_on_oom = 0\n# \u6587\u4ef6\u53e5\u67c4\u9650\u5236\nfs.file-max = 52706963\n# \u7f51\u7edc\u5c42\u9762\u4f18\u5316\nnet.core.somaxconn = 32768\nnet.core.netdev_max_backlog = 16384\nnet.ipv4.tcp_max_syn_backlog = 16384\nEOF\nsysctl --system\n</code></pre> <pre><code>tee -a /etc/security/limits.conf &lt;&lt;EOF\n* soft nofile 1048576\n* hard nofile 1048576\n* soft nproc 1048576\n* hard nproc 1048576\n* soft memlock unlimited\n* hard memlock unlimited\nEOF\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#_2","title":"\u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u65f6","text":""},{"location":"k8s/installation/kubeadm-v1.28/#1-docker","title":"1. \u5b89\u88c5docker","text":"<p><pre><code># step 1: \u5b89\u88c5\u5fc5\u8981\u7684\u4e00\u4e9b\u7cfb\u7edf\u5de5\u5177\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\n\n# step 2: \u4fe1\u4efb Docker \u7684 GPG \u516c\u94a5\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Step 3: \u5199\u5165\u8f6f\u4ef6\u6e90\u4fe1\u606f\necho \\\n\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\nsudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\napt-get update\n\n# Step 4: \u5b89\u88c5Docker\napt-get update\napt-get install docker-ce=5:24.0.6-1~ubuntu.20.04~focal\n</code></pre> <pre><code>tee  /etc/docker/daemon.json &lt;&lt; 'EOF'\n{\n    \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n    \"registry-mirrors\": [\n         \"https://o4uba187.mirror.aliyuncs.com\",\n         \"https://docker.1ms.run\",\n         \"https://docker.1panel.live\"\n    ]\n}\nEOF\nsystemctl daemon-reload\napt-cache madison docker-ce\nsystemctl restart docker &amp;&amp; systemctl enable docker\n</code></pre></p>"},{"location":"k8s/installation/kubeadm-v1.28/#2-cri-dockerd","title":"2. \u5b89\u88c5cri-dockerd","text":"<p><pre><code>#curl -LO https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.10/cri-dockerd_0.3.10.3-0.ubuntu-focal_amd64.deb\ncurl -LO https://ghfast.top/https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.10/cri-dockerd_0.3.10.3-0.ubuntu-focal_amd64.deb\napt install  -y ./cri-dockerd_0.3.10.3-0.ubuntu-focal_amd64.deb\n</code></pre> <pre><code>cp /lib/systemd/system/cri-docker.service{,.bak}\nsed -i 's@ExecStart=/usr/bin/cri-dockerd@ExecStart=/usr/bin/cri-dockerd --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 @g' /usr/lib/systemd/system/cri-docker.service\nsystemctl daemon-reload\nsystemctl restart cri-docker &amp;&amp; systemctl  enable  cri-docker\n</code></pre></p>"},{"location":"k8s/installation/kubeadm-v1.28/#kubernetes","title":"\u5b89\u88c5Kubernetes\u96c6\u7fa4","text":""},{"location":"k8s/installation/kubeadm-v1.28/#1_1","title":"1. \u5b89\u88c5\u8f6f\u4ef6\u5305","text":"<pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https\ncurl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/Release.key |\ngpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/ /\" |\ntee /etc/apt/sources.list.d/kubernetes.list\n\napt-get update\n# apt-cache madison kubeadm\napt install kubeadm=1.28.10-1.1 kubelet=1.28.10-1.1 kubectl=1.28.10-1.1 -y\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#2-kubelet","title":"2. \u914d\u7f6ekubelet","text":"<pre><code>sed -i 's@KUBELET_EXTRA_ARGS=@KUBELET_EXTRA_ARGS=\"--cgroup-driver=systemd\"@' /etc/sysconfig/kubelet\nsystemctl enable kubelet\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#3","title":"3. \u521d\u59cb\u5316\u96c6\u7fa4","text":"<ul> <li>master\u8282\u70b9\u521d\u59cb\u5316</li> </ul> <p><pre><code># kubeadm config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --kubernetes-version=1.28.10    --cri-socket=unix:///var/run/cri-dockerd.sock\nkubeadm init \\\n--kubernetes-version=1.28.10 \\\n--control-plane-endpoint=\"k8s-vip\" \\\n--image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\\n--pod-network-cidr=10.244.0.0/16 \\\n--service-cidr=10.96.0.0/12 \\\n--token-ttl=0 \\\n--upload-certs \\\n--cri-socket=unix:///var/run/cri-dockerd.sock Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\nmkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\nkubeadm join k8s-vip:6443 --token r4818h.uij0z3hsaxdnhxbc \\\n--discovery-token-ca-cert-hash sha256:1db36f3ec9ef152e106bbf94e0ba9afc8235ce4f1b6a1cfddd2016c87023d320 \\\n--control-plane --certificate-key f7e2789c31298a2b4ac17e2079c6125a0395e701ef0cd3a6e82c959be3c49ae0\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n\"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join k8s-vip:6443 --token r4818h.uij0z3hsaxdnhxbc \\\n--discovery-token-ca-cert-hash sha256:1db36f3ec9ef152e106bbf94e0ba9afc8235ce4f1b6a1cfddd2016c87023d320\n</code></pre> - \u914d\u7f6ekubectl\u5ba2\u6237\u7aef</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\nkubectl  get nodes\nNAME           STATUS     ROLES           AGE   VERSION\nk8s-master01   NotReady   control-plane   55s   v1.28.10\n</code></pre> <ul> <li>\u52a0\u5165worker\u8282\u70b9</li> </ul> <pre><code>kubeadm join k8s-vip:6443 --token r4818h.uij0z3hsaxdnhxbc \\\n--discovery-token-ca-cert-hash sha256:1db36f3ec9ef152e106bbf94e0ba9afc8235ce4f1b6a1cfddd2016c87023d320 \\\n--cri-socket=unix:///var/run/cri-dockerd.sock\n\nkubectl  get nodes\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m22s   v1.28.10\nk8s-worker01   NotReady   &lt;none&gt;          34s     v1.28.10\nk8s-worker02   NotReady   &lt;none&gt;          11s     v1.28.10\nk8s-worker03   NotReady   &lt;none&gt;          7s      v1.28.10\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#4_1","title":"4. \u90e8\u7f72\u7f51\u7edc\u63d2\u4ef6","text":"<p>\u7248\u672c\u5bf9\u5e94\u5173\u7cfb</p> <p>\u9700\u6ce8\u610f\u7248\u672c\u5bf9\u5e94\u5173\u7cfb\uff1ahttps://docs.tigera.io/calico/3.27/getting-started/kubernetes/requirements</p> <pre><code># curl https://raw.githubusercontent.com/projectcalico/calico/v3.27.5/manifests/calico.yaml -O\ncurl -O https://ghfast.top/https://raw.githubusercontent.com/projectcalico/calico/v3.27.5/manifests/calico.yaml\nsed -i 's@# - name: CALICO_IPV4POOL_CIDR@- name: CALICO_IPV4POOL_CIDR@g' calico.yaml\nsed -i 's@#   value: \"192.168.0.0/16\"@  value: \"10.244.0.0/16\"@g' calico.yaml\nkubectl  apply -f calico.yaml\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#5-ipvs_1","title":"5. \u4fee\u6539\u7f51\u7edc\u6a21\u578b\u4e3aIPVS","text":"<pre><code>kubectl  edit cm/kube-proxy -n kube-system -o yaml\n  ...\n  metricsBindAddress: \"\"\nmode: \"ipvs\" #\nnodePortAddresses: null\n  oomScoreAdj: null\n  ...\n\nkubectl -n kube-system rollout restart daemonset kube-proxy\n ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.96.0.1:443 rr\n  -&gt; 192.168.1.201:6443           Masq    1      1          0\nTCP  10.96.0.10:53 rr\n  -&gt; 10.244.39.193:53             Masq    1      0          0\n-&gt; 10.244.39.194:53             Masq    1      0          0\nTCP  10.96.0.10:9153 rr\n  -&gt; 10.244.39.193:9153           Masq    1      0          0\n-&gt; 10.244.39.194:9153           Masq    1      0          0\nUDP  10.96.0.10:53 rr\n  -&gt; 10.244.39.193:53             Masq    1      0          0\n-&gt; 10.244.39.194:53             Masq    1      0          0\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#_3","title":"\u9a8c\u8bc1\u96c6\u7fa4","text":""},{"location":"k8s/installation/kubeadm-v1.28/#1_2","title":"1. \u9a8c\u8bc1\u7f51\u7edc","text":"<p><pre><code>kubectl  create deployment myapp --image=ikubernetes/myapp:v1 --replicas=5\nkubectl  get pod -o wide\nNAME                     READY   STATUS    RESTARTS   AGE     IP              NODE           NOMINATED NODE   READINESS GATES\nmyapp-5d9c4b4647-27m7j   1/1     Running   0          9m21s   10.244.69.195   k8s-worker02   &lt;none&gt;           &lt;none&gt;\nmyapp-5d9c4b4647-2p6fs   1/1     Running   0          9m21s   10.244.79.70    k8s-worker01   &lt;none&gt;           &lt;none&gt;\nmyapp-5d9c4b4647-4dr5g   1/1     Running   0          3s      10.244.69.197   k8s-worker02   &lt;none&gt;           &lt;none&gt;\nmyapp-5d9c4b4647-p24mm   1/1     Running   0          9m21s   10.244.79.68    k8s-worker01   &lt;none&gt;           &lt;none&gt;\nmyapp-5d9c4b4647-vlsft   1/1     Running   0          28s     10.244.39.201   k8s-worker03   &lt;none&gt;           &lt;none&gt;\n</code></pre> <pre><code>kubectl expose deployment myapp --port=80 --target-port=80 --type=\"NodePort\" kubectl  get svc/myapp\nNAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nmyapp   NodePort   10.99.179.210   &lt;none&gt;        80:32178/TCP   20s\n</code></pre> <pre><code>for i in `seq 5`;do curl 10.99.179.210/hostname.html;done\nmyapp-5d9c4b4647-2p6fs\nmyapp-5d9c4b4647-p24mm\nmyapp-5d9c4b4647-4dr5g\nmyapp-5d9c4b4647-27m7j\nmyapp-5d9c4b4647-vlsft\n</code></pre></p>"},{"location":"k8s/installation/kubeadm-v1.28/#2-dns","title":"2. \u9a8c\u8bc1dns","text":"<pre><code>apt install -y dnsutils\n\n~# kubectl get svc -n kube-system\nNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\nkube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   44m\nroot@k8s-master01:~# dig -t A myapp.default.svc.cluster.local @10.96.0.10\n\n; &lt;&lt;&gt;&gt; DiG 9.18.30-0ubuntu0.20.04.2-Ubuntu &lt;&lt;&gt;&gt; -t A myapp.default.svc.cluster.local @10.96.0.10\n;; global options: +cmd\n;; Got answer:\n;; WARNING: .local is reserved for Multicast DNS\n;; You are currently testing what happens when an mDNS query is leaked to DNS\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 45538\n;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n;; WARNING: recursion requested but not available\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n; COOKIE: fd1fdbe4c0177389 (echoed)\n;; QUESTION SECTION:\n;myapp.default.svc.cluster.local. IN    A\n\n;; ANSWER SECTION:\nmyapp.default.svc.cluster.local. 30 IN  A   10.99.179.210\n\n;; Query time: 0 msec\n;; SERVER: 10.96.0.10#53(10.96.0.10) (UDP)\n;; WHEN: Sat Apr 26 18:44:31 CST 2025\n;; MSG SIZE  rcvd: 119\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#add-on","title":"Add-On\u63d2\u4ef6","text":""},{"location":"k8s/installation/kubeadm-v1.28/#1-kuboard","title":"1. kuboard","text":"<p>\u4f7f\u7528 hostPath \u63d0\u4f9b\u6301\u4e45\u5316\u5b58\u50a8\uff0c\u5c06 kuboard \u6240\u4f9d\u8d56\u7684 Etcd \u90e8\u7f72\u5230 Master \u8282\u70b9\uff0c\u5e76\u5c06 etcd \u7684\u6570\u636e\u76ee\u5f55\u6620\u5c04\u5230 Master \u8282\u70b9\u7684\u672c\u5730\u76ee\u5f55\uff1b\u63a8</p> <pre><code>kubectl apply -f https://addons.kuboard.cn/kuboard/kuboard-v3-swr.yaml\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#2-metrics-server","title":"2. metrics-server","text":"<p>\u901a\u8fc7 kuboard \u5b89\u88c5 metrics-server</p> <pre><code>kubectl  top nodes\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nk8s-master01   133m         6%     1904Mi          24%\nk8s-worker01   112m         5%     2325Mi          29%\nk8s-worker02   63m          3%     1862Mi          23%\nk8s-worker03   80m          4%     1965Mi          25%\nkubectl  top pods\nNAME                     CPU(cores)   MEMORY(bytes)\nmyapp-5d9c4b4647-27m7j   0m           2Mi\nmyapp-5d9c4b4647-2p6fs   0m           2Mi\nmyapp-5d9c4b4647-4dr5g   0m           2Mi\nmyapp-5d9c4b4647-p24mm   0m           2Mi\nmyapp-5d9c4b4647-vlsft   0m           2Mi\nroot@k8s-master01:~#\n</code></pre>"},{"location":"k8s/installation/kubeadm-v1.28/#_4","title":"\u5378\u8f7d\u96c6\u7fa4","text":"<ul> <li>\u5378\u8f7d\u6574\u4e2a\u96c6\u7fa4</li> </ul> <p>\u5148\u62c6\u9664\u5404\u4e2a\u5de5\u4f5c\u8282\u70b9\uff0c\u5728\u62c6\u9664\u63a7\u5236\u5e73\u9762</p> <pre><code>kubeadm reset  --cri-socket=unix:///var/run/cri-dockerd.sock\nrm -rf /etc/kubernetes /var/lib/kubelet/ /var/lib/cni/ /etc/cni/net.d/ /var/lib/etcd/\n</code></pre> <ul> <li>\u62c6\u9664\u5355\u4e2a\u5de5\u4f5c\u8282\u70b9</li> </ul> <pre><code># 1. \u7981\u6b62\u8c03\u5ea6\nkubectl cordon k8s-worker03\n# 2. \u6392\u7a7a\u8282\u70b9\nkubectl drain k8s-worker03\n# 3. \u5220\u9664\u8282\u70b9\nkubectl delete node  k8s-worker03\n# 4. \u6267\u884creset\u8ddf\u540e\u7eed\u7684\u6e05\u7406\u5de5\u4f5c\nkubeadm reset  --cri-socket=unix:///var/run/cri-dockerd.sock\nrm -rf /etc/kubernetes /var/lib/kubelet/ /var/lib/cni/ /etc/cni/net.d/ /var/lib/etcd/\n</code></pre>"},{"location":"k8s/pod/podinfo/","title":"\u5065\u5eb7\u68c0\u67e5","text":""},{"location":"k8s/pod/podinfo/#_1","title":"\u76d1\u6d4b\u7c7b\u578b","text":""},{"location":"k8s/pod/podinfo/#startup-probe","title":"startup Probe","text":"<p>\u5224\u65adPod\u5185\u5bb9\u5668\u662f\u5426\u542f\u52a8\u5b8c\u6210</p>"},{"location":"k8s/pod/podinfo/#liveness-probe","title":"liveness Probe","text":"<p>\u5224\u65ad Pods \u662f\u5426\u5b58\u6d3b</p>"},{"location":"k8s/pod/podinfo/#readiness-probe","title":"readiness Probe","text":"<p>\u5bb9\u5668\u6240\u5728 Pod \u4e0a\u62a5\u8fd8\u672a\u5c31\u7eea\u7684\u4fe1\u606f\uff0c\u5e76\u4e14\u4e0d\u63a5\u53d7\u901a\u8fc7 Kubernetes Service \u7684\u6d41\u91cf\u3002</p>"},{"location":"k8s/pod/podinfo/#_2","title":"\u76d1\u6d4b\u673a\u5236","text":""},{"location":"k8s/pod/podinfo/#exec-action","title":"Exec Action","text":""},{"location":"k8s/pod/podinfo/#tcpsocket-action","title":"TcpSocket Action","text":""},{"location":"k8s/pod/podinfo/#httpget-action","title":"HTTPGet Action","text":""},{"location":"postgres/%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C/","title":"\u5b89\u88c5\u624b\u518c","text":""},{"location":"postgres/%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C/#_1","title":"\u4e8c\u8fdb\u5236\u5305\u5b89\u88c5","text":"<p>\u5b98\u7f51\uff1a https://www.postgresql.org/download/linux/redhat/</p> <pre><code># \u5b89\u88c5 RPM \u5b58\u50a8\u5e93\uff1a\nsudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm\n\nsudo yum install -y postgresql14-server\n\nsudo /usr/pgsql-14/bin/postgresql-14-setup initdb\nsudo systemctl enable  postgresql-14 --now\n~]# ss -tnlp|grep 5432\nLISTEN     0      128    127.0.0.1:5432                     *:*                   users:((\"postmaster\",pid=2410,fd=7))\nLISTEN     0      128      [::1]:5432                  [::]:*                   users:((\"postmaster\",pid=2410,fd=6))\n</code></pre>"},{"location":"postgres/%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C/#_2","title":"\u6e90\u7801\u5b89\u88c5","text":"<p>\u5b98\u7f51\uff1ahttps://www.postgresql.org/docs/14/install-short.html</p> <pre><code># \u5b89\u88c5\u4f9d\u8d56\nyum install -y gcc make readline-devel zlib-devel bison flex libxml2-devel libxslt-devel openldap-devel pam-devel python3-devel\n# \u89e3\u538b\u7f16\u8bd1\nwget https://ftp.postgresql.org/pub/source/v14.17/postgresql-14.17.tar.gz\ntar -xf postgresql-14.17.tar.gz  &amp;&amp; cd postgresql-14.17\n./configure --prefix=/usr/local/pgsql-14.17  --with-pgport=5432\nmake -j 2 world\nmake install-world\n# \u521b\u5efa\u7528\u6237\nuseradd  -m -d /home/postgres -s /bin/bash postgres\n# echo -e 'postgres123\\npostgres123'|passwd  postgres\necho postgres:123456|chpasswd # \u521b\u5efa\u6570\u636e\u76ee\u5f55\u5e76\u6388\u6743\nmkdir /usr/local/pgsql-14.17/data\nchown postgres:postgres /usr/local/pgsql-14.17/data\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\ncat &gt; /etc/profile.d/pgsql.sh &lt;&lt; 'EOF'\nexport PGHOME=/usr/local/pgsql-14.17/\nexport PATH=$PGHOME/bin:$PATH\nexport PGDATA=$PGHOME/data\nexport PGUSER=postgres\nexport MANPATH=$MANPATH:/usr/local/pgsql-14.17/share/man/\nEOF\n# \u521d\u59cb\u5316\u6570\u636e\u5e93\nsu - postgres\n/usr/local/pgsql-14.17/bin/initdb -D /usr/local/pgsql-14.17/data\n# \u542f\u52a8\u6570\u636e\u5e93\n/usr/local/pgsql-14.17/bin/pg_ctl -D /usr/local/pgsql-14.17/data -l logfile start\n\n# \u67e5\u770b\u6570\u636e\u5e93\u7248\u672c\n~]$ psql\npsql (14.17)\nType \"help\" for help.\n\npostgres=# select version();\nversion                                                  ----------------------------------------------------------------------------------------------------------\n PostgreSQL 14.17 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44), 64-bit\n(1 row)\n</code></pre>"},{"location":"postgres/%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C/#_3","title":"\u914d\u7f6e\u8fdc\u7a0b\u8fde\u63a5","text":"<p>postgresql.conf<pre><code># \u4fee\u6539\u4e3b\u914d\u7f6e\u6587\u4ef6\uff0c\u76d1\u542c\u6240\u6709IP\nvim /usr/local/pgsql-14.17/data/postgresql.conf listen_addresses = '*'\n# \u9700\u91cd\u542f\u670d\u52a1\u751f\u6548\npg_ctl  restart\n</code></pre> pg_hba.conf<pre><code># \u4fee\u6539\u8bbf\u95ee\u63a7\u5236\u6587\u4ef6\uff0c\u8fdc\u7a0b\u4e3b\u673a\u8fdb\u8fc7md5\u5bc6\u7801\u9a8c\u8bc1\u8fde\u63a5\u6240\u6709\u6570\u636e\u5e93\nvim /usr/local/pgsql-14.17/data/pg_hba.conf\n# TYPE  DATABASE        USER            ADDRESS                 METHOD\n# IPv4 remote connections:\nhost    all             all             0.0.0.0/0               md5\n# \u9700\u91cd\u542f\u670d\u52a1\u751f\u6548\npg_ctl  restart\n</code></pre> <pre><code># \u4e3a postgres \u7528\u6237\u6dfb\u52a0\u5bc6\u7801\n~]$ psql psql (14.17)\nType \"help\" for help.\n\npostgres=# ALTER USER postgres with password '123456';\nALTER ROLE\n\n~]$ psql -h 192.168.0.118 db1 postgres\nPassword for user postgres: psql (14.17)\nType \"help\" for help.\n\ndb1=# \\c\nYou are now connected to database \"db1\" as user \"postgres\".\n</code></pre></p>"},{"location":"postgres/%E5%B8%B8%E8%A7%81PSQL%E8%AF%AD%E5%8F%A5/","title":"\u5e38\u89c1PSQL\u8bed\u53e5","text":"<ul> <li>\u67e5\u770bpsql\u7248\u672c</li> </ul> <pre><code> ~]# su - postgres\n-bash-4.2$ psql -c \"SELECT version();\"\nversion                                                  ----------------------------------------------------------------------------------------------------------\n PostgreSQL 14.17 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44), 64-bit\n</code></pre>"},{"location":"prometheus/quick_start/","title":"\u5feb\u901f\u90e8\u7f72Prometheus\u76d1\u63a7\u7cfb\u7edf","text":""},{"location":"prometheus/quick_start/#prometheus-server","title":"Prometheus Server","text":""},{"location":"prometheus/quick_start/#1prometheus","title":"1.\u4e0b\u8f7dprometheus","text":"<pre><code># wget https://github.com/prometheus/prometheus/releases/download/v2.47.0/prometheus-2.47.0.linux-amd64.tar.gz\nwget https://ghfast.top/https://github.com/prometheus/prometheus/releases/download/v2.47.0/prometheus-2.47.0.linux-amd64.tar.gz\ntar -xf prometheus-2.47.0.linux-amd64.tar.gz cd prometheus-2.47.0.linux-amd64\n</code></pre>"},{"location":"prometheus/quick_start/#2","title":"2. \u521b\u5efa\u76f8\u5173\u7528\u6237\u548c\u76ee\u5f55","text":"<pre><code>mkdir -p /etc/prometheus /var/lib/prometheus\nuseradd --no-create-home --shell /bin/false prometheus\ncp prometheus promtool /usr/local/bin/\ncp -r consoles console_libraries /etc/prometheus\ncp prometheus.yml /etc/prometheus\nchown -R prometheus:prometheus /etc/prometheus /var/lib/prometheus\n</code></pre>"},{"location":"prometheus/quick_start/#3-prometheus","title":"3. \u914d\u7f6e Prometheus","text":"<pre><code>vim /etc/prometheus/prometheus.yml\n\nglobal:\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\nevaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n# scrape_timeout is set to the global default (10s).\n# Alertmanager configuration\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          # - alertmanager:9093\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\nrule_files:\n  # - \"first_rules.yml\"\n# - \"second_rules.yml\"\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.\n- job_name: \"prometheus\"\n# metrics_path defaults to '/metrics'\n# scheme defaults to 'http'.\nstatic_configs:\n      - targets: [\"localhost:9090\"]\n</code></pre>"},{"location":"prometheus/quick_start/#4","title":"4. \u521b\u5efa\u7cfb\u7edf\u670d\u52a1\u6587\u4ef6","text":"<p>\u4e3a\u4e86\u65b9\u4fbf\u7ba1\u7406 Prometheus \u670d\u52a1\u7684\u542f\u52a8\u3001\u505c\u6b62\u548c\u91cd\u542f\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u7cfb\u7edf\u670d\u52a1\u6587\u4ef6\u3002</p> <pre><code>cat &gt; /etc/systemd/system/prometheus.service &lt;&lt; 'EOF'\n[Unit]\nDescription=Prometheus\nWants=network-online.target\nAfter=network-online.target\n[Service]\nUser=prometheus\nGroup=prometheus\nType=simple\nExecStart=/usr/local/bin/prometheus \\\n    --config.file /etc/prometheus/prometheus.yml \\\n    --storage.tsdb.path /var/lib/prometheus/ \\\n    --web.console.templates=/etc/prometheus/consoles \\\n    --web.console.libraries=/etc/prometheus/console_libraries \\\n    --web.enable-lifecycle \\\n    --storage.tsdb.retention.time=180d\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"prometheus/quick_start/#5","title":"5. \u542f\u52a8\u670d\u52a1","text":"<pre><code>systemctl  daemon-reload &amp;&amp; systemctl  enable prometheus --now\n ss -tnlp|grep 9090\nLISTEN     0      128       [::]:9090                  [::]:*                   users:((\"prometheus\",pid=15231,fd=7))\n</code></pre>"},{"location":"prometheus/quick_start/#6-prometheus","title":"6. \u6dfb\u52a0\u5230Prometheus\u76d1\u63a7","text":"<p>prometheus.yml<pre><code>...\nscrape_configs:\n  ...\n  - job_name: \"promehtues-node01\"\nstatic_configs:\n      - targets: [\"192.168.1.68:9100\"]\n- job_name: \"promehtues-node02\"\nstatic_configs:\n      - targets: [\"192.168.1.69:9100\"]                                   </code></pre> <pre><code>curl -X POST 127.0.0.1:9090/-/reload\n</code></pre></p>"},{"location":"prometheus/quick_start/#node-exporter","title":"Node Exporter","text":""},{"location":"prometheus/quick_start/#1-node-exporter","title":"1. \u4e0b\u8f7d Node Exporter","text":"<pre><code>#wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz\nwget https://ghfast.top/https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz\ntar -xzf node_exporter-1.6.1.linux-amd64.tar.gz\ncd node_exporter-1.6.1.linux-amd64\n</code></pre>"},{"location":"prometheus/quick_start/#2_1","title":"2. \u521b\u5efa\u7528\u6237","text":"<p>\u4e3a\u5b89\u5168\u8d77\u89c1\uff0c\u521b\u5efa\u4e00\u4e2a\u4e13\u95e8\u7684\u7528\u6237\u6765\u8fd0\u884c Node Exporter</p> <pre><code>useradd --no-create-home --shell /bin/false node_exporter\ncp node_exporter /usr/local/bin\nchown node_exporter:node_exporter /usr/local/bin/node_exporter\n</code></pre>"},{"location":"prometheus/quick_start/#3","title":"3.  \u521b\u5efa\u7cfb\u7edf\u670d\u52a1","text":"<pre><code>cat &gt; /etc/systemd/system/node_exporter.service &lt;&lt; 'EOF'\n[Unit]\nDescription=Node Exporter\nWants=network-online.target\nAfter=network-online.target\n[Service]\nUser=node_exporter\nGroup=node_exporter\nType=simple\nExecStart=/usr/local/bin/node_exporter\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"prometheus/quick_start/#4_1","title":"4. \u542f\u52a8\u670d\u52a1","text":"<pre><code>systemctl daemon-reload &amp;&amp; systemctl enable node_exporter --now\nss -tnlp|grep 9100\nLISTEN     0      128       [::]:9100                  [::]:*                   users:((\"node_exporter\",pid=12535,fd=3))\n</code></pre>"},{"location":"prometheus/quick_start/#grafana","title":"Grafana","text":"<pre><code>yum install -y https://dl.grafana.com/oss/release/grafana-11.6.1-1.x86_64.rpm\nsystemctl  enable  grafana-server.service --now\n</code></pre>"},{"location":"prometheus/quick_start/#alert-manager","title":"Alert Manager","text":""}]}